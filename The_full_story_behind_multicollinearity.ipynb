{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Linear regression model\n",
        "\n",
        "$\\hat{y} = Xw + b$\n",
        "\n",
        "The **goal** is to find the weights $w$ and bias $b$ such that, given features of a new data sample from the same distribution as $X$, the new sample's label will be predicted correctly with the smallest error.\n",
        "\n",
        "# Loss\n",
        "\n",
        "Our goal is: $w^*, b^* = {argmin}_{w, b} L(w, b)$\n",
        "\n",
        "After we differentialte $||y - Xw||^2$ w.r.t. $w$, we end up with\n",
        "\n",
        "# Optimal parameters\n",
        "\n",
        "$w^* = (X^TX)^{-1} X^Ty$\n",
        "\n",
        "Solving for $w^*$ provides us with the optimal solution for the optimization problem. However, it only works when the matrix $X^TX$ is invertible (has full rank - the columns are linearly independent).\n",
        "\n",
        "**IF** it does not have full rank -> some of the features are linearly dependent. In a perfect world $X^TX$ would be a diagonal (or nearly diagonal) square matrix (i.e. no linearly dependent variables).\n",
        "\n",
        "# Why do we need it to have full rank?\n",
        "\n",
        "In $w^*$ we have $(X^TX)^{-1}$ - inversing the matrix. To inverse a matrix, the matrix needs to satisfy:\n",
        "\n",
        "1. Be a square matrix - $(X^TX)^{-1}$ returns a square matrix so we are *good*\n",
        "2. Have non-zero determinant. In the case of square matrices having a non-zero determinant is equivalent to being full rank\n",
        "  * if features are linearly independent -> we are *good*\n",
        "  * if features are linearly dependent -> we are *not* good\n",
        "\n",
        "# What do we do when there is linear dependence or multicullinearity ?\n",
        "\n",
        "We cannot directly solve for $w^*$. We need to employ alternative methods to estimate it. We need to get an invertible $X^TX$, so we can:\n",
        "* use QR decomposition - useful even when $X^TX$ is not invertible\n",
        "* SVD = obtain a pseudo-inverse $X^TX$\n",
        "* regularization - helps mitigate multicollinearity\n",
        "* other methods\n",
        "\n",
        "Even though there is multicollinearity, we can still run a regression and obtain w estimates. Any if you ask an LLM, read a textbook, most likely it will leave at: thes estimates are bad and unreliable.\n",
        "\n",
        "# Why ?\n",
        "\n",
        "Multicollinearity means 'near-singularity' of the design matrix (matrix showing relationships between Xs and y).\n",
        "\n",
        "# Why do we care for near-singularity (which is equivalent to near 0 determinant) ?\n",
        "\n",
        "It means that we might have a near 0 eigenvalue. For a square matrix to have a determinant of (near) 0, at least one of its rows or columns must be a linear combination of the other rows or columns. This loss of dimensionality leads to a collapsing or compression of space, which is reflected in the eigenvalue being (near) 0.\n",
        "\n",
        "# Why do we care if there is a (near) 0 eigenvalue ?\n",
        "\n",
        "It suggests there is some strong linear relationship or multicollinearity among the features because of the way they relate to the `rank` and `condition number` of the matrix $X^TX$."
      ],
      "metadata": {
        "id": "E8iYU6SixeuV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example of a matrix's eigenvalues when **there is** multicollinearity"
      ],
      "metadata": {
        "id": "FJ_vh80b9cL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(suppress=True)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "M17huicts7sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = np.array([1, 2, 3, 4, 5])\n",
        "x2 = np.array([2, 4, 6, 8, 10])\n",
        "\n",
        "X = np.column_stack((x1, x2))\n",
        "\n",
        "XTX = np.dot(X.T, X)\n",
        "\n",
        "eigenvalues_XTX = np.linalg.eigvals(XTX)\n",
        "\n",
        "print('Multicollinearity case:')\n",
        "print(\"X^TX:\")\n",
        "print(XTX)\n",
        "print(\"Eigenvalues of X^TX:\")\n",
        "print(eigenvalues_XTX)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfLwwFOK990C",
        "outputId": "4e3c9b72-c69d-48f1-c658-ec74fa057951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multicollinearity case:\n",
            "X^TX:\n",
            "[[ 55 110]\n",
            " [110 220]]\n",
            "Eigenvalues of X^TX:\n",
            "[  0. 275.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example of a matrix's eigenvalues when there is **no** multicollinearity"
      ],
      "metadata": {
        "id": "47TPY7LA_WjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = np.array([1, 2, 3, 4, 5])\n",
        "x2 = np.array([3, 5, 4, 6, 7])\n",
        "\n",
        "X = np.column_stack((x1, x2))\n",
        "\n",
        "XTX = np.dot(X.T, X)\n",
        "\n",
        "eigenvalues_XTX = np.linalg.eigvals(XTX)\n",
        "\n",
        "print('No multicollinearity case:')\n",
        "print(\"X^TX:\")\n",
        "print(XTX)\n",
        "print(\"Eigenvalues of X^TX:\")\n",
        "print(eigenvalues_XTX)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqTuEWkc9-JQ",
        "outputId": "ff07ebf7-c95d-413b-9331-af470ef9aff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No multicollinearity case:\n",
            "X^TX:\n",
            "[[ 55  84]\n",
            " [ 84 135]]\n",
            "Eigenvalues of X^TX:\n",
            "[  1.9623732 188.0376268]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How do eigenvalues relate to rank ?\n",
        "\n",
        "On the one hand, if a matrix has full rank (from the start: this is one of the conditions in order to inverse a matrix) that means all its eigenvalues are non-zero. Each eigenvalue of a matrix corresponds to a factor by which a non-zero eigenvector is stretched or shrunk when the matrix is applied to it. If all rows and columns are linearly independent, then the matrix will not collapse any direction onto another during a transformation. Since there are no linearly dependent directions to collapse onto zero, all the eigenvalues must be non-zero.\n",
        "\n",
        "On the other hand, a matrix with less than full rank means there is some linear dependence - there exists one direction in the input space that is mapped to 0 in the output space -> at least one 0 eigenvalue."
      ],
      "metadata": {
        "id": "4MGnFZqT_lkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example of a matrix's rank when **there is** multicollinearity"
      ],
      "metadata": {
        "id": "DD0rxp0HCmME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# same matrix as before\n",
        "x1 = np.array([1, 2, 3, 4, 5])\n",
        "x2 = np.array([2, 4, 6, 8, 10])\n",
        "\n",
        "X = np.column_stack((x1, x2))\n",
        "\n",
        "XTX = np.dot(X.T, X)\n",
        "\n",
        "eigenvalues = np.linalg.eigvals(XTX)\n",
        "\n",
        "print('Multicollinearity case:')\n",
        "print(f'{eigenvalues=}')\n",
        "\n",
        "rank = np.linalg.matrix_rank(XTX)\n",
        "print(\"rank of the matrix:\", rank) # supposed to be 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PRP2RJF_lIu",
        "outputId": "7be4394b-e4a2-4f26-a2a4-684e8ca741ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multicollinearity case:\n",
            "eigenvalues=array([  0., 275.])\n",
            "rank of the matrix: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example of a matrix's eigenvalues when there is **no** multicollinearity"
      ],
      "metadata": {
        "id": "wxx_3yg1DRE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# same matrix as before\n",
        "x1 = np.array([1, 2, 3, 4, 5])\n",
        "x2 = np.array([3, 5, 4, 6, 7])\n",
        "\n",
        "X = np.column_stack((x1, x2))\n",
        "\n",
        "XTX = np.dot(X.T, X)\n",
        "\n",
        "eigenvalues_XTX = np.linalg.eigvals(XTX)\n",
        "\n",
        "print('No multicollinearity case:')\n",
        "print(\"X^TX:\")\n",
        "print(XTX)\n",
        "print(\"Eigenvalues of X^TX:\")\n",
        "print(eigenvalues_XTX)\n",
        "\n",
        "rank = np.linalg.matrix_rank(XTX)\n",
        "print(\"rank of the matrix:\", rank) # supposed to be 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pI-8nBvl9-Lc",
        "outputId": "bd1eaed3-7396-48ae-e2b9-e5bb6db2de59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No multicollinearity case:\n",
            "X^TX:\n",
            "[[ 55  84]\n",
            " [ 84 135]]\n",
            "Eigenvalues of X^TX:\n",
            "[  1.9623732 188.0376268]\n",
            "rank of the matrix: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How do eigenvalues relate to the matrix's condition number ?\n",
        "\n",
        "The condition number of a matrix is a measure of its sensitivity to pertributions or changes in the elements of the matrix; or how small changes in the elements of the matrix might affect its behaviour or properties, such as its eigenvalues or the solutions to linear systems involving that matrix.\n",
        "\n",
        "The spread of the eigenvalues can give insight into the matrix's conditioning - smaller spread generally suggests better conditioning.\n",
        "\n",
        "In most cases, a larger condition number indicates poorer conditioning, while a smaller - better conditioning."
      ],
      "metadata": {
        "id": "yItCKFPSD5nW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example of a matrix's rank when **there is** multicollinearity"
      ],
      "metadata": {
        "id": "qYbc-HIiFS0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# same matrix as before\n",
        "x1 = np.array([1, 2, 3, 4, 5])\n",
        "x2 = np.array([2, 4, 6, 8, 10])\n",
        "\n",
        "X = np.column_stack((x1, x2))\n",
        "\n",
        "XTX = np.dot(X.T, X)\n",
        "\n",
        "eigenvalues = np.linalg.eigvals(XTX)\n",
        "\n",
        "print('Multicollinearity case:')\n",
        "print(f'{eigenvalues=}')\n",
        "\n",
        "rank = np.linalg.matrix_rank(XTX)\n",
        "print(\"rank of the matrix:\", rank) # supposed to be 2\n",
        "\n",
        "condition_number = np.linalg.cond(XTX)\n",
        "print(f'{condition_number=}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7ROBD4r9-Nf",
        "outputId": "be09c26d-99e0-4a9f-d04f-52413f7e8c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multicollinearity case:\n",
            "eigenvalues=array([  0., 275.])\n",
            "rank of the matrix: 1\n",
            "condition_number=inf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example of a matrix's eigenvalues when there is **no** multicollinearity"
      ],
      "metadata": {
        "id": "Dn88MlaVFejo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# same matrix as before\n",
        "x1 = np.array([1, 2, 3, 4, 5])\n",
        "x2 = np.array([3, 5, 4, 6, 7])\n",
        "\n",
        "X = np.column_stack((x1, x2))\n",
        "\n",
        "XTX = np.dot(X.T, X)\n",
        "\n",
        "eigenvalues_XTX = np.linalg.eigvals(XTX)\n",
        "\n",
        "print('No multicollinearity case:')\n",
        "print(\"X^TX:\")\n",
        "print(XTX)\n",
        "print(\"Eigenvalues of X^TX:\")\n",
        "print(eigenvalues_XTX)\n",
        "\n",
        "rank = np.linalg.matrix_rank(XTX)\n",
        "print(\"rank of the matrix:\", rank) # supposed to be 2\n",
        "\n",
        "condition_number = np.linalg.cond(XTX)\n",
        "print(f'{condition_number=}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGY0KIue9-Px",
        "outputId": "7e65867a-ef9d-474f-8a04-3da14769f445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No multicollinearity case:\n",
            "X^TX:\n",
            "[[ 55  84]\n",
            " [ 84 135]]\n",
            "Eigenvalues of X^TX:\n",
            "[  1.9623732 188.0376268]\n",
            "rank of the matrix: 2\n",
            "condition_number=95.82154225314864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building up to multicollinearity\n",
        "\n",
        "* Eigenvalues and Eigenvectors: when an eigenvalue is close to 0, it indicates that the associated eigenvector does not scale much during the applied transformation\n",
        "* Minimal scaling: the vector does not scale (much), this means the transformation has very little effect along the direction of that eigenvector (little effect on the features or characteristics of the data that are aligned with that particular eigenvector)\n",
        "* Redundancy: if an eigenvector is nearly parallel to the null space of the transformation matrix, this suggests there is a linear combination of the data (features in the design matrix) that has no effect on the outcome\n",
        "* Multicollinearity: redundancy (or near-redundancy) in the data is a characteristic of multicollinearity - one predictor variable can be approximately expressed as a linear combination of the others. If we have multicollinearity, there is near linear dependence and the matrix is (near) singular, which makes $X^TX$ ill-conditioned and numerically unstable (sensitive to pertrubations) which inflates the parameter variances, resulting in poor estimates\n",
        "\n",
        "# Fro a 0 eigenvalue to multicollinearity\n",
        "\n",
        "A 0 eigenvalue indicates minimal scaling of the corresponding eigenvector which suggests redundancy or near-redundancy in the data. This redundancy implies a high degree of correlation between predictor variables, leading to multicollinearity in the regression model.\n"
      ],
      "metadata": {
        "id": "OVLupYkyFxZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example of linear regression when **there is** multicollinearity"
      ],
      "metadata": {
        "id": "bWw_KPQaFxAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# same as before\n",
        "x1 = np.array([1, 2, 3, 4, 5])\n",
        "x2 = np.array([2, 4, 6, 8, 10])\n",
        "\n",
        "X = np.column_stack((x1, x2))\n",
        "\n",
        "XTX = np.dot(X.T, X)\n",
        "\n",
        "eigenvalues = np.linalg.eigvals(XTX)\n",
        "\n",
        "print('Multicollinearity case:')\n",
        "print(f'{eigenvalues=}')\n",
        "\n",
        "rank = np.linalg.matrix_rank(XTX)\n",
        "print(\"Rank of the matrix:\", rank)\n",
        "\n",
        "condition_number = np.linalg.cond(XTX)\n",
        "print(f'{condition_number=}')\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "true_coef = [2, 1]\n",
        "\n",
        "y = np.dot(X, true_coef) + np.random.normal(0, 0.1, size=len(x1))\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "print(\"\\nOriginal Coefficients and Intercept:\")\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "\n",
        "X_perturbed = X + 0.001 * np.random.randn(*X.shape)\n",
        "\n",
        "model_perturbed = LinearRegression()\n",
        "model_perturbed.fit(X_perturbed, y)\n",
        "\n",
        "print(\"\\nPerturbed Coefficients and Intercept:\")\n",
        "print(\"Coefficients:\", model_perturbed.coef_)\n",
        "print(\"Intercept:\", model_perturbed.intercept_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REIw9H6z9-US",
        "outputId": "ddbab390-29dd-4493-e5a1-4b92518dd319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multicollinearity case:\n",
            "eigenvalues=array([  0., 275.])\n",
            "Rank of the matrix: 1\n",
            "condition_number=inf\n",
            "\n",
            "Original Coefficients and Intercept:\n",
            "Coefficients: [0.80373975 1.6074795 ]\n",
            "Intercept: -0.09077696986945405\n",
            "\n",
            "Perturbed Coefficients and Intercept:\n",
            "Coefficients: [2.11403896 0.95241435]\n",
            "Intercept: -0.09155051004990256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example of linear regression when there is **no** multicollinearity"
      ],
      "metadata": {
        "id": "OxdySkUhM0bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# same as before\n",
        "x1 = np.array([1, 2, 3, 4, 5])\n",
        "x2 = np.array([3, 5, 4, 6, 7])\n",
        "\n",
        "X = np.column_stack((x1, x2))\n",
        "\n",
        "XTX = np.dot(X.T, X)\n",
        "\n",
        "eigenvalues = np.linalg.eigvals(XTX)\n",
        "\n",
        "print('No multicollinearity case:')\n",
        "print(f'{eigenvalues=}')\n",
        "\n",
        "rank = np.linalg.matrix_rank(XTX)\n",
        "print(\"Rank of the matrix:\", rank)\n",
        "\n",
        "condition_number = np.linalg.cond(XTX)\n",
        "print(f'{condition_number=}')\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "true_coef = [2, 1]\n",
        "\n",
        "y = np.dot(X, true_coef) + np.random.normal(0, 0.1, size=len(x1))\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "print(\"\\nOriginal Coefficients and Intercept:\")\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "\n",
        "X_perturbed = X + 0.001 * np.random.randn(*X.shape)\n",
        "\n",
        "model_perturbed = LinearRegression()\n",
        "model_perturbed.fit(X_perturbed, y)\n",
        "\n",
        "print(\"\\nPerturbed Coefficients and Intercept:\")\n",
        "print(\"Coefficients:\", model_perturbed.coef_)\n",
        "print(\"Intercept:\", model_perturbed.intercept_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTgfyTcKI_AH",
        "outputId": "f0dc38bc-d8d5-4ca9-f95c-70fcc31610b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No multicollinearity case:\n",
            "eigenvalues=array([  1.9623732, 188.0376268])\n",
            "Rank of the matrix: 2\n",
            "condition_number=95.82154225314864\n",
            "\n",
            "Original Coefficients and Intercept:\n",
            "Coefficients: [2.13761981 0.80053492]\n",
            "Intercept: 0.5920131994987106\n",
            "\n",
            "Perturbed Coefficients and Intercept:\n",
            "Coefficients: [2.13756268 0.80067638]\n",
            "Intercept: 0.5915172479972988\n"
          ]
        }
      ]
    }
  ]
}